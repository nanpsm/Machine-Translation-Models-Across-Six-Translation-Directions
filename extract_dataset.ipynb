{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "id": "NFaK7jHvyBwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re\n",
        "import pandas as pd\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "zhCdr3k4z6c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random seed\n",
        "SEED = 42\n",
        "\n",
        "#Total samples per pair\n",
        "N_TOTAL = 10000\n",
        "\n",
        "# Split ratio\n",
        "TRAIN_FRAC = 0.80\n",
        "VAL_FRAC   = 0.10\n",
        "TEST_FRAC  = 0.10\n",
        "\n",
        " # Basic length filters (language-agnostic)\n",
        "MIN_CHARS = 3\n",
        "MAX_CHARS_SRC = 500\n",
        "MAX_CHARS_TGT = 500"
      ],
      "metadata": {
        "id": "sTc_MnfhyFvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"Untranslated / misaligned\" filters\n",
        "# If target is too similar to source, it often means untranslated or wrong alignment\n",
        "SIMILARITY_JACCARD_THRESHOLD = 0.85   # 0..1 (higher = stricter)"
      ],
      "metadata": {
        "id": "gba3GYrq8LkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Korean validation\n",
        "KO_MIN_HANGUL_CHARS = 1\n",
        "KO_MIN_HANGUL_RATIO = 0.30            # hangul chars / total non-space chars\n",
        "KO_MAX_LATIN_RATIO  = 0.50            # if mostly latin letters, likely wrong"
      ],
      "metadata": {
        "id": "4wxdtw2s9U3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vietnamese validation (Latin script; use diacritics as weak signal + anti-copy)\n",
        "VI_MIN_DIACRITIC_RATIO = 0.005        # if 0, allow no diacritics; small value filters obvious English\n",
        "VI_MAX_COPY_RATIO      = 0.90         # additional anti-copy (token overlap proxy)"
      ],
      "metadata": {
        "id": "qOVM_G8y8SL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Indonesian validation (Latin script; anti-copy + optional \"English-ness\" heuristic)\n",
        "ID_MAX_COPY_RATIO      = 0.90"
      ],
      "metadata": {
        "id": "1Gz--mUN8UhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output folder\n",
        "OUT_ROOT = \"dataset_splits_opus100_10k\""
      ],
      "metadata": {
        "id": "oL2C3NA38Kao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_clean(text: str) -> str:\n",
        "    # consistent whitespace cleanup\n",
        "    return \" \".join(str(text).strip().split())"
      ],
      "metadata": {
        "id": "jNjFAWYnyLRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_word_re = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+|[0-9]+\", re.UNICODE)"
      ],
      "metadata": {
        "id": "sYz_5pnJ8d8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_words(text: str):\n",
        "    return _word_re.findall(text.lower())"
      ],
      "metadata": {
        "id": "R4mh5YBs8eku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(a: str, b: str) -> float:\n",
        "    wa = set(tokenize_words(a))\n",
        "    wb = set(tokenize_words(b))\n",
        "    if not wa and not wb:\n",
        "        return 1.0\n",
        "    if not wa or not wb:\n",
        "        return 0.0\n",
        "    inter = len(wa & wb)\n",
        "    union = len(wa | wb)\n",
        "    return inter / max(union, 1)"
      ],
      "metadata": {
        "id": "In-58uz98khq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ratio_latin_letters(text: str) -> float:\n",
        "    chars = [c for c in text if not c.isspace()]\n",
        "    if not chars:\n",
        "        return 0.0\n",
        "    latin = sum(1 for c in chars if (\"A\" <= c <= \"Z\") or (\"a\" <= c <= \"z\"))\n",
        "    return latin / len(chars)"
      ],
      "metadata": {
        "id": "QHAXQmJ78lKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_hangul(text: str) -> int:\n",
        "    # Hangul syllables + jamo ranges\n",
        "    n = 0\n",
        "    for c in text:\n",
        "        o = ord(c)\n",
        "        if (0xAC00 <= o <= 0xD7A3) or (0x1100 <= o <= 0x11FF) or (0x3130 <= o <= 0x318F):\n",
        "            n += 1\n",
        "    return n"
      ],
      "metadata": {
        "id": "lD92BpG_8oQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hangul_ratio(text: str) -> float:\n",
        "    chars = [c for c in text if not c.isspace()]\n",
        "    if not chars:\n",
        "        return 0.0\n",
        "    return count_hangul(text) / len(chars)"
      ],
      "metadata": {
        "id": "uoiSZe7l8qXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def diacritic_ratio(text: str) -> float:\n",
        "    # Vietnamese often contains diacritics; English almost never does.\n",
        "    # Count non-ascii letters as a proxy (good enough for filtering obvious English in VI target).\n",
        "    chars = [c for c in text if not c.isspace()]\n",
        "    if not chars:\n",
        "        return 0.0\n",
        "    non_ascii = sum(1 for c in chars if ord(c) > 127)\n",
        "    return non_ascii / len(chars)"
      ],
      "metadata": {
        "id": "KzDkFXMr8r6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pass_basic_filters(src: str, tgt: str) -> bool:\n",
        "    if not src or not tgt:\n",
        "        return False\n",
        "    if len(src) < MIN_CHARS or len(tgt) < MIN_CHARS:\n",
        "        return False\n",
        "    if len(src) > MAX_CHARS_SRC or len(tgt) > MAX_CHARS_TGT:\n",
        "        return False\n",
        "    # Remove exact copies (common misalignment/untranslated)\n",
        "    if src.strip() == tgt.strip():\n",
        "        return False\n",
        "    # Remove near-copies (untranslated / same sentence)\n",
        "    if jaccard_similarity(src, tgt) >= SIMILARITY_JACCARD_THRESHOLD:\n",
        "        return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "kAvxKtiF8sy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pass_language_validation(src_lang: str, tgt_lang: str, src: str, tgt: str) -> bool:\n",
        "    # We validate mainly the TARGET language (tgt_lang), because that’s where you saw English leakage.\n",
        "    # For bidirectional, you’ll swap columns at training time; we keep canonical EN->X datasets clean.\n",
        "    if tgt_lang == \"ko\":\n",
        "        # Must contain Hangul and have reasonable Hangul dominance\n",
        "        if count_hangul(tgt) < KO_MIN_HANGUL_CHARS:\n",
        "            return False\n",
        "        if hangul_ratio(tgt) < KO_MIN_HANGUL_RATIO:\n",
        "            return False\n",
        "        if ratio_latin_letters(tgt) > KO_MAX_LATIN_RATIO:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    if tgt_lang == \"vi\":\n",
        "        # Vietnamese uses Latin letters, so we can’t script-check.\n",
        "        # Filter obvious English leakage using diacritics ratio + anti-copy.\n",
        "        if jaccard_similarity(src, tgt) > VI_MAX_COPY_RATIO:\n",
        "            return False\n",
        "        # Mild filter: require *some* non-ascii characters occasionally.\n",
        "        # (Set VI_MIN_DIACRITIC_RATIO to 0.0 if you find it removes too much.)\n",
        "        if diacritic_ratio(tgt) < VI_MIN_DIACRITIC_RATIO:\n",
        "            # allow if it still looks non-copy (avoid removing short valid VI with no diacritics)\n",
        "            # keep this conservative:\n",
        "            if len(tgt.split()) >= 6:   # longer lines should usually show diacritics\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if tgt_lang == \"id\":\n",
        "        # Indonesian uses Latin letters too; rely on anti-copy.\n",
        "        if jaccard_similarity(src, tgt) > ID_MAX_COPY_RATIO:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    # Default: accept\n",
        "    return True"
      ],
      "metadata": {
        "id": "6axaEdwB8u1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_splits_opus100(pair_cfg: str, src_lang: str, tgt_lang: str, out_dir: str):\n",
        "    print(f\"\\n=== Loading opus100/{pair_cfg} for {src_lang}->{tgt_lang} ===\")\n",
        "    raw = load_dataset(\"opus100\", pair_cfg, split=\"train\")\n",
        "\n",
        "    # Map to clean source/target\n",
        "    def _map_row(ex):\n",
        "        src = basic_clean(ex[\"translation\"][src_lang])\n",
        "        tgt = basic_clean(ex[\"translation\"][tgt_lang])\n",
        "        return {\"source\": src, \"target\": tgt}\n",
        "\n",
        "    ds = raw.map(_map_row, remove_columns=raw.column_names)\n",
        "\n",
        "    # Apply basic + language validation filters\n",
        "    def _filter_row(ex):\n",
        "        s = ex[\"source\"]\n",
        "        t = ex[\"target\"]\n",
        "        if not pass_basic_filters(s, t):\n",
        "            return False\n",
        "        if not pass_language_validation(src_lang, tgt_lang, s, t):\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    ds = ds.filter(_filter_row)\n",
        "\n",
        "    # Deterministic shuffle + fixed size\n",
        "    ds = ds.shuffle(seed=SEED)\n",
        "    n_total = min(N_TOTAL, len(ds))\n",
        "    ds = ds.select(range(n_total))\n",
        "\n",
        "    # 80/10/10 split by slicing (deterministic)\n",
        "    n_train = int(n_total * TRAIN_FRAC)\n",
        "    n_val   = int(n_total * VAL_FRAC)\n",
        "    n_test  = n_total - n_train - n_val\n",
        "\n",
        "    train_ds = ds.select(range(0, n_train))\n",
        "    val_ds   = ds.select(range(n_train, n_train + n_val))\n",
        "    test_ds  = ds.select(range(n_train + n_val, n_train + n_val + n_test))\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    pd.DataFrame(train_ds).to_csv(os.path.join(out_dir, \"train.csv\"), index=False, encoding=\"utf-8\")\n",
        "    pd.DataFrame(val_ds).to_csv(os.path.join(out_dir, \"val.csv\"), index=False, encoding=\"utf-8\")\n",
        "    pd.DataFrame(test_ds).to_csv(os.path.join(out_dir, \"test.csv\"), index=False, encoding=\"utf-8\")\n",
        "\n",
        "    # Metadata for reproducibility\n",
        "    with open(os.path.join(out_dir, \"meta.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"dataset=opus100\\n\")\n",
        "        f.write(f\"pair_cfg={pair_cfg}\\n\")\n",
        "        f.write(f\"direction={src_lang}->{tgt_lang}\\n\")\n",
        "        f.write(f\"seed={SEED}\\n\")\n",
        "        f.write(f\"n_total={n_total}\\n\")\n",
        "        f.write(f\"split={TRAIN_FRAC}/{VAL_FRAC}/{TEST_FRAC}\\n\")\n",
        "        f.write(f\"min_chars={MIN_CHARS}\\n\")\n",
        "        f.write(f\"max_chars_src={MAX_CHARS_SRC}\\n\")\n",
        "        f.write(f\"max_chars_tgt={MAX_CHARS_TGT}\\n\")\n",
        "        f.write(f\"jaccard_threshold={SIMILARITY_JACCARD_THRESHOLD}\\n\")\n",
        "        f.write(f\"ko_min_hangul_chars={KO_MIN_HANGUL_CHARS}\\n\")\n",
        "        f.write(f\"ko_min_hangul_ratio={KO_MIN_HANGUL_RATIO}\\n\")\n",
        "        f.write(f\"ko_max_latin_ratio={KO_MAX_LATIN_RATIO}\\n\")\n",
        "        f.write(f\"vi_min_diacritic_ratio={VI_MIN_DIACRITIC_RATIO}\\n\")\n",
        "        f.write(f\"vi_max_copy_ratio={VI_MAX_COPY_RATIO}\\n\")\n",
        "        f.write(f\"id_max_copy_ratio={ID_MAX_COPY_RATIO}\\n\")\n",
        "\n",
        "    print(f\"After filtering: total={n_total} | train={len(train_ds)} val={len(val_ds)} test={len(test_ds)}\")\n",
        "    print(f\"Saved to: {out_dir}\")"
      ],
      "metadata": {
        "id": "-NY1M546yTQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create three frozen datasets\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)\n",
        "\n",
        "# We store EN->X as the canonical saved files.\n",
        "# For X->EN, your training code should swap columns (source<->target) without making new files.\n",
        "\n",
        "make_splits_opus100(\"en-id\", \"en\", \"id\", os.path.join(OUT_ROOT, \"en_id\"))\n",
        "make_splits_opus100(\"en-vi\", \"en\", \"vi\", os.path.join(OUT_ROOT, \"en_vi\"))\n",
        "make_splits_opus100(\"en-ko\", \"en\", \"ko\", os.path.join(OUT_ROOT, \"en_ko\"))\n",
        "\n",
        "print(\"\\nDONE  Clean + frozen dataset splits created.\")\n",
        "print(f\"Folder: {OUT_ROOT}/  (upload this folder to Drive)\")"
      ],
      "metadata": {
        "id": "JtXMWxi4ydYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EDsQyCFI006R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r dataset_splits_opus100_10k /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "dWox1f1h04LV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}