{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U transformers datasets evaluate sacrebleu accelerate sentencepiece"
      ],
      "metadata": {
        "id": "d5evBficYpgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install sacremoses"
      ],
      "metadata": {
        "id": "3QHYQeXzljFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Od-9jxMO71_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, json\n",
        "import numpy as np\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        ")"
      ],
      "metadata": {
        "id": "OPeWIvLPPEj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ROOT = \"/content/drive/MyDrive/dataset_splits_opus100_10k\"\n",
        "OUT_ROOT  = \"/content/drive/MyDrive/results_marianmt\"\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)"
      ],
      "metadata": {
        "id": "f2xba-MnPGlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "chrf = evaluate.load(\"chrf\")"
      ],
      "metadata": {
        "id": "-1x0Y12sPH6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAIR_TO_MODEL = {\n",
        "    (\"en\",\"id\"): \"Helsinki-NLP/opus-mt-en-id\",\n",
        "    (\"id\",\"en\"): \"Helsinki-NLP/opus-mt-id-en\",\n",
        "    (\"en\",\"vi\"): \"Helsinki-NLP/opus-mt-en-vi\",\n",
        "    (\"vi\",\"en\"): \"Helsinki-NLP/opus-mt-vi-en\",\n",
        "    (\"en\",\"ko\"): \"Helsinki-NLP/opus-mt-tc-big-en-ko\",\n",
        "    (\"ko\",\"en\"): \"Helsinki-NLP/opus-mt-ko-en\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "Tt0UCuhEPJ7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAIR_TO_FOLDER = {\n",
        "    (\"en\",\"id\"): \"en_id\",\n",
        "    (\"id\",\"en\"): \"en_id\",\n",
        "    (\"en\",\"vi\"): \"en_vi\",\n",
        "    (\"vi\",\"en\"): \"en_vi\",\n",
        "    (\"en\",\"ko\"): \"en_ko\",\n",
        "    (\"ko\",\"en\"): \"en_ko\",\n",
        "}"
      ],
      "metadata": {
        "id": "YL8olgLYPKeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_frozen_split(folder_name: str) -> DatasetDict:\n",
        "    train_path = os.path.join(DATA_ROOT, folder_name, \"train.csv\")\n",
        "    val_path   = os.path.join(DATA_ROOT, folder_name, \"val.csv\")\n",
        "    test_path  = os.path.join(DATA_ROOT, folder_name, \"test.csv\")\n",
        "\n",
        "    ds_train = load_dataset(\"csv\", data_files=train_path, split=\"train\")\n",
        "    ds_val   = load_dataset(\"csv\", data_files=val_path, split=\"train\")\n",
        "    ds_test  = load_dataset(\"csv\", data_files=test_path, split=\"train\")\n",
        "\n",
        "    return DatasetDict(train=ds_train, validation=ds_val, test=ds_test)"
      ],
      "metadata": {
        "id": "0rTc3mSOPOLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_swap_columns(ds: DatasetDict, reverse: bool) -> DatasetDict:\n",
        "    if not reverse:\n",
        "        return ds\n",
        "    def _swap(ex):\n",
        "        return {\"source\": ex[\"target\"], \"target\": ex[\"source\"]}\n",
        "    return DatasetDict(\n",
        "        train=ds[\"train\"].map(_swap),\n",
        "        validation=ds[\"validation\"].map(_swap),\n",
        "        test=ds[\"test\"].map(_swap)\n",
        "    )"
      ],
      "metadata": {
        "id": "RGVXtA77PTQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(ds: DatasetDict, tokenizer, max_len=128) -> DatasetDict:\n",
        "    def _tok(batch):\n",
        "        model_inputs = tokenizer(\n",
        "            batch[\"source\"], max_length=max_len, truncation=True\n",
        "        )\n",
        "        labels = tokenizer(\n",
        "              text_target=batch[\"target\"],\n",
        "              max_length=max_len,\n",
        "              truncation=True,\n",
        "          )\n",
        "\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "\n",
        "    return DatasetDict(\n",
        "        train=ds[\"train\"].map(_tok, batched=True, remove_columns=ds[\"train\"].column_names),\n",
        "        validation=ds[\"validation\"].map(_tok, batched=True, remove_columns=ds[\"validation\"].column_names),\n",
        "        test=ds[\"test\"].map(_tok, batched=True, remove_columns=ds[\"test\"].column_names),\n",
        "    )"
      ],
      "metadata": {
        "id": "U-WvRlxfPW1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions(model, tokenizer, texts, max_len=128, batch_size=16):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
        "        enc = {k: v.to(model.device) for k, v in enc.items()}\n",
        "        with torch.no_grad():\n",
        "            out = model.generate(**enc, max_length=max_len)\n",
        "        preds.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
        "    return preds"
      ],
      "metadata": {
        "id": "E2krkvWJPZP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_bleu_chrf(preds, refs):\n",
        "    bleu_score = bleu.compute(predictions=preds, references=[[r] for r in refs])[\"score\"]\n",
        "    chrf_score = chrf.compute(predictions=preds, references=[[r] for r in refs])[\"score\"]\n",
        "    return float(bleu_score), float(chrf_score)\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "3BIIDnYHPdDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_marianmt_direction(src_lang: str, tgt_lang: str,\n",
        "                          epochs=2, batch_size=16, lr=5e-5,\n",
        "                          max_len=128, sample_n=10):\n",
        "    assert (src_lang, tgt_lang) in PAIR_TO_MODEL\n",
        "\n",
        "    model_name = PAIR_TO_MODEL[(src_lang, tgt_lang)]\n",
        "    folder_name = PAIR_TO_FOLDER[(src_lang, tgt_lang)]\n",
        "    reverse = (src_lang != \"en\")  # because saved datasets are EN->X canonical\n",
        "\n",
        "    run_id = f\"{src_lang}_to_{tgt_lang}_marianmt\"\n",
        "    out_dir = os.path.join(OUT_ROOT, run_id)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # ---- Load frozen data ----\n",
        "    ds = load_frozen_split(folder_name)\n",
        "    ds = maybe_swap_columns(ds, reverse=reverse)\n",
        "\n",
        "    # ---- Load model/tokenizer ----\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    model = model.float()\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "    # ---- Baseline inference on test set ----\n",
        "    test_src = ds[\"test\"][\"source\"]\n",
        "    test_ref = ds[\"test\"][\"target\"]\n",
        "\n",
        "    t0 = time.time()\n",
        "    baseline_preds = []\n",
        "    for i in range(0, len(test_src), batch_size):\n",
        "        batch = test_src[i:i+batch_size]\n",
        "        enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "        with torch.no_grad():\n",
        "            out = model.generate(**enc, max_length=max_len)\n",
        "        baseline_preds.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
        "    baseline_time = time.time() - t0\n",
        "\n",
        "    baseline_bleu, baseline_chrf = eval_bleu_chrf(baseline_preds, test_ref)\n",
        "\n",
        "    # ---- Tokenize for fine-tuning ----\n",
        "    tok_ds = tokenize_dataset(ds, tokenizer, max_len=max_len)\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "    # ---- Trainer ---\n",
        "    args = Seq2SeqTrainingArguments(\n",
        "    output_dir=os.path.join(out_dir, \"checkpoints\"),\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    save_total_limit=1,\n",
        "\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    report_to=\"none\",\n",
        "    seed=42,\n",
        "\n",
        "    label_smoothing_factor=0.1,\n",
        "    )\n",
        "\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tok_ds[\"train\"],\n",
        "        eval_dataset=tok_ds[\"validation\"],\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # ---- Fine-tune ----\n",
        "    train_t0 = time.time()\n",
        "    train_output = trainer.train()\n",
        "    train_time = time.time() - train_t0\n",
        "\n",
        "    model = trainer.model\n",
        "    model.eval()\n",
        "\n",
        "    # ---- After-FT inference on test set ----\n",
        "    t1 = time.time()\n",
        "    finetuned_preds = []\n",
        "    for i in range(0, len(test_src), batch_size):\n",
        "        batch = test_src[i:i+batch_size]\n",
        "        enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "        with torch.no_grad():\n",
        "            out = model.generate(**enc, max_length=max_len)\n",
        "        finetuned_preds.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
        "    finetuned_time = time.time() - t1\n",
        "\n",
        "    finetuned_bleu, finetuned_chrf = eval_bleu_chrf(finetuned_preds, test_ref)\n",
        "\n",
        "    # ---- Save metrics ----\n",
        "    gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    metrics = {\n",
        "        \"direction\": f\"{src_lang}->{tgt_lang}\",\n",
        "        \"model\": model_name,\n",
        "        \"dataset_folder\": folder_name,\n",
        "        \"reverse_columns_used\": bool(reverse),\n",
        "        \"max_len\": max_len,\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": lr,\n",
        "        \"gpu\": gpu_name,\n",
        "        \"n_params\": int(n_params),\n",
        "        \"baseline\": {\n",
        "            \"bleu\": baseline_bleu,\n",
        "            \"chrf\": baseline_chrf,\n",
        "            \"inference_time_sec\": float(baseline_time),\n",
        "        },\n",
        "        \"finetuned\": {\n",
        "            \"bleu\": finetuned_bleu,\n",
        "            \"chrf\": finetuned_chrf,\n",
        "            \"inference_time_sec\": float(finetuned_time),\n",
        "        },\n",
        "        \"train_time_sec\": float(train_time),\n",
        "        \"trainer_log_history\": trainer.state.log_history,\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(out_dir, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    # ---- Save sample outputs for error analysis ----\n",
        "    idx = np.linspace(0, len(test_src)-1, num=min(sample_n, len(test_src)), dtype=int).tolist()\n",
        "    samples = []\n",
        "    for k in idx:\n",
        "        samples.append({\n",
        "            \"source\": test_src[k],\n",
        "            \"reference\": test_ref[k],\n",
        "            \"before_ft\": baseline_preds[k],\n",
        "            \"after_ft\": finetuned_preds[k],\n",
        "        })\n",
        "\n",
        "    import pandas as pd\n",
        "    pd.DataFrame(samples).to_csv(os.path.join(out_dir, \"samples_before_after.csv\"), index=False, encoding=\"utf-8\")\n",
        "\n",
        "    # Save final model (optional, but useful)\n",
        "    model.save_pretrained(os.path.join(out_dir, \"final_model\"))\n",
        "    tokenizer.save_pretrained(os.path.join(out_dir, \"final_model\"))\n",
        "\n",
        "    print(f\"Done {src_lang}->{tgt_lang}\")\n",
        "    print(f\"Baseline:  BLEU={baseline_bleu:.2f}, chrF={baseline_chrf:.2f}\")\n",
        "    print(f\"Fine-tuned: BLEU={finetuned_bleu:.2f}, chrF={finetuned_chrf:.2f}\")\n",
        "    print(f\"Saved to: {out_dir}\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "F-F6eaH8O-FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"Helsinki-NLP/opus-mt-tc-big-en-ko\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\").eval()\n",
        "\n",
        "texts = [\n",
        "    \"Are you seeing anyone?\",\n",
        "    \"We need to find cover now.\",\n",
        "    \"I can't believe you did that.\",\n",
        "    \"This is not what I expected.\",\n",
        "    \"Where are you going tonight?\"\n",
        "]\n",
        "\n",
        "enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=96).to(\"cuda\")\n",
        "out = model.generate(**enc, max_length=96, num_beams=4)\n",
        "print(tokenizer.batch_decode(out, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "6NNExU8wXSQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directions = [(\"en\",\"id\"), (\"id\",\"en\"),\n",
        "              (\"en\",\"vi\"), (\"vi\",\"en\"),\n",
        "              (\"en\",\"ko\"), (\"ko\",\"en\")]\n",
        "\n",
        "all_metrics = []\n",
        "for src, tgt in directions:\n",
        "    all_metrics.append(run_marianmt_direction(src, tgt, epochs=1, batch_size=8, lr=1e-5, max_len=96))"
      ],
      "metadata": {
        "id": "tGtV9tCjPmk8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}